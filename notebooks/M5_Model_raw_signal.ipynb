{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from common import *\n",
    "from dataset import ArrhythmiaDataset\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "RECORD_DIR_PATH = '../data/mit-bih-arrhythmia-database-1.0.0'\n",
    "WINDOW_SIZE = 540\n",
    "MOVING_AVERAGE_RANGE = None\n",
    "INCLUDE_MANUAL_LABELS = False\n",
    "SUBSET_FROM_MANUAL_LABELS = True\n",
    "INCLUDE_RAW_SIGNAL = True\n",
    "CLASSES = ['N', 'L', 'R', 'a', 'V', 'J', 'F'] if SUBSET_FROM_MANUAL_LABELS else ['N', 'L', 'R', 'A', 'a', 'V', 'j', 'J', 'E', 'f', 'F', '[', '!', ']', '/', 'x', '|', 'Q']\n",
    "\n",
    "n_epoch = 15\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "RUN_NAME = 'raw_signal'\n",
    "CHECKPOINT_PATH = f'../models/{RUN_NAME} - checkpoint.pt'\n",
    "ACCURACY_MOVING_AVERAGE_SIZE = 30  # moving average for accuracy to check if performance degraded\n",
    "\n",
    "# TODO: S, e - need some preprocessing, dimensions seem to be wrong in one of these\n",
    "# TODO: Q - of course, quite confusing, this is the most confused beat in confusion matrices"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Randomness seed\n",
    "random_seed = 1 # or any of your favorite number\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename='V.csv' Unique_keys=50\n",
      "filename='L.csv' Unique_keys=100\n",
      "filename='R.csv' Unique_keys=150\n",
      "filename='J.csv' Unique_keys=218\n",
      "filename='a.csv' Unique_keys=271\n",
      "filename='F.csv' Unique_keys=321\n",
      "filename='N.csv' Unique_keys=371\n",
      "filename='100.atr' patient_record_number=100\n",
      "beat_slice_array.shape=(38, 1080) beat_slices.shape=torch.Size([38, 1080])\n",
      "filename='124.atr' patient_record_number=124\n",
      "beat_slice_array.shape=(20, 1080) beat_slices.shape=torch.Size([20, 1080])\n",
      "self.data.shape=torch.Size([38, 1080]) beat_slices.shape=torch.Size([20, 1080])\n",
      "filename='111.atr' patient_record_number=111\n",
      "beat_slice_array.shape=(50, 1080) beat_slices.shape=torch.Size([50, 1080])\n",
      "self.data.shape=torch.Size([58, 1080]) beat_slices.shape=torch.Size([50, 1080])\n",
      "filename='208.atr' patient_record_number=208\n",
      "beat_slice_array.shape=(50, 1080) beat_slices.shape=torch.Size([50, 1080])\n",
      "self.data.shape=torch.Size([108, 1080]) beat_slices.shape=torch.Size([50, 1080])\n",
      "filename='101.atr' patient_record_number=101\n",
      "beat_slice_array.shape=(11, 1080) beat_slices.shape=torch.Size([11, 1080])\n",
      "self.data.shape=torch.Size([158, 1080]) beat_slices.shape=torch.Size([11, 1080])\n",
      "filename='201.atr' patient_record_number=201\n",
      "beat_slice_array.shape=(53, 1080) beat_slices.shape=torch.Size([53, 1080])\n",
      "self.data.shape=torch.Size([169, 1080]) beat_slices.shape=torch.Size([53, 1080])\n",
      "filename='118.atr' patient_record_number=118\n",
      "beat_slice_array.shape=(50, 1080) beat_slices.shape=torch.Size([50, 1080])\n",
      "self.data.shape=torch.Size([222, 1080]) beat_slices.shape=torch.Size([50, 1080])\n",
      "filename='200.atr' patient_record_number=200\n",
      "beat_slice_array.shape=(50, 1080) beat_slices.shape=torch.Size([50, 1080])\n",
      "self.data.shape=torch.Size([272, 1080]) beat_slices.shape=torch.Size([50, 1080])\n",
      "filename='234.atr' patient_record_number=234\n",
      "beat_slice_array.shape=(48, 1080) beat_slices.shape=torch.Size([48, 1080])\n",
      "self.data.shape=torch.Size([322, 1080]) beat_slices.shape=torch.Size([48, 1080])\n",
      "torch.Size([370, 1080])\n",
      "370\n"
     ]
    }
   ],
   "source": [
    "dataset = ArrhythmiaDataset(RECORD_DIR_PATH, WINDOW_SIZE, only_include_labels = CLASSES, moving_average_range = MOVING_AVERAGE_RANGE, include_manual_labels = INCLUDE_MANUAL_LABELS, subset_from_manual_labels = SUBSET_FROM_MANUAL_LABELS, include_raw_signal =\n",
    "INCLUDE_RAW_SIGNAL)\n",
    "\n",
    "print(dataset.data.shape)\n",
    "print(len(dataset.labels))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V: 50\n",
      "L: 50\n",
      "F: 50\n",
      "J: 68\n",
      "N: 49\n",
      "a: 53\n",
      "R: 50\n"
     ]
    }
   ],
   "source": [
    "labels, counts = torch.unique(dataset.labels_encoded, dim = 0, return_counts = True)\n",
    "\n",
    "for label, count in zip(labels, counts):\n",
    "    print(f'{dataset.get_label_from_tensor(label)}: {count}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([370, 1080])\n",
      "370\n",
      "V: 50\n",
      "L: 50\n",
      "F: 50\n",
      "J: 68\n",
      "N: 49\n",
      "a: 53\n",
      "R: 50\n"
     ]
    }
   ],
   "source": [
    "# Drop some Normal beats to balance classes\n",
    "normal_beat_mask = np.array(dataset.labels) == 'N'\n",
    "\n",
    "new_labels = []\n",
    "for idx, l in enumerate(normal_beat_mask):\n",
    "    # Leave 100% samples\n",
    "    if l and random.uniform(0, 1) < 1:\n",
    "        normal_beat_mask[idx] = False\n",
    "    if not normal_beat_mask[idx]:\n",
    "        new_labels.append(dataset.labels[idx])\n",
    "\n",
    "new_data = dataset.data[normal_beat_mask == False]\n",
    "dataset.data = new_data\n",
    "dataset.labels = new_labels\n",
    "dataset.encode_labels()\n",
    "\n",
    "def show_class_count(dataset: ArrhythmiaDataset):\n",
    "    print(dataset.data.shape)\n",
    "    print(len(dataset.labels))\n",
    "    labels, counts = torch.unique(dataset.labels_encoded, dim = 0, return_counts = True)\n",
    "\n",
    "    for label, count in zip(labels, counts):\n",
    "        print(f'{dataset.get_label_from_tensor(label)}: {count}')\n",
    "\n",
    "show_class_count(dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN DATASET:\n",
      "torch.Size([296, 1080])\n",
      "0\n",
      "V: 40\n",
      "L: 40\n",
      "F: 40\n",
      "J: 54\n",
      "N: 39\n",
      "a: 43\n",
      "R: 40\n",
      "TEST DATASET:\n",
      "torch.Size([74, 1080])\n",
      "0\n",
      "V: 10\n",
      "L: 10\n",
      "F: 10\n",
      "J: 14\n",
      "N: 10\n",
      "a: 10\n",
      "R: 10\n"
     ]
    }
   ],
   "source": [
    "def collate_fn(batch):\n",
    "\n",
    "    # A data tuple has the form:\n",
    "    # waveform, one-hot-encoded_label\n",
    "\n",
    "    tensors, targets = [], []\n",
    "\n",
    "    # Gather in lists, and encode labels as indices\n",
    "    for waveform, label in batch:\n",
    "        tensors += [waveform]\n",
    "        targets += [label]\n",
    "\n",
    "    # Group the list of tensors into a batched tensor\n",
    "    tensors = torch.stack(tensors)\n",
    "    tensors = tensors[:, None, :]\n",
    "    targets = torch.stack(targets)\n",
    "\n",
    "    return tensors, targets\n",
    "\n",
    "\n",
    "\n",
    "if device == \"cuda\":\n",
    "    num_workers = 1\n",
    "    pin_memory = True\n",
    "else:\n",
    "    num_workers = 0\n",
    "    pin_memory = False\n",
    "\n",
    "train_dataset, test_dataset = dataset.train_test_split(0.2)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    ")\n",
    "\n",
    "print('TRAIN DATASET:')\n",
    "show_class_count(train_dataset)\n",
    "\n",
    "print('TEST DATASET:')\n",
    "show_class_count(test_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M5(\n",
      "  (conv1): Conv1d(1, 32, kernel_size=(3,), stride=(1,))\n",
      "  (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv1d(32, 32, kernel_size=(3,), stride=(1,))\n",
      "  (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv1d(32, 32, kernel_size=(3,), stride=(1,))\n",
      "  (bn3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool3): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv4): Conv1d(32, 64, kernel_size=(3,), stride=(1,))\n",
      "  (bn4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool4): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv5): Conv1d(64, 64, kernel_size=(3,), stride=(1,))\n",
      "  (bn5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool5): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv6): Conv1d(64, 64, kernel_size=(3,), stride=(1,))\n",
      "  (bn6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool6): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=7, bias=True)\n",
      ")\n",
      "Number of parameters: 40135\n"
     ]
    }
   ],
   "source": [
    "class M5(nn.Module):\n",
    "    def __init__(self, n_input=1, n_output=35, stride=1, n_channel=32):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(n_input, n_channel, kernel_size=3, stride=stride)\n",
    "        self.bn1 = nn.BatchNorm1d(n_channel)\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "        self.conv2 = nn.Conv1d(n_channel, n_channel, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm1d(n_channel)\n",
    "        self.pool2 = nn.MaxPool1d(2)\n",
    "        self.conv3 = nn.Conv1d(n_channel, n_channel, kernel_size=3)\n",
    "        self.bn3 = nn.BatchNorm1d(n_channel)\n",
    "        self.pool3 = nn.MaxPool1d(3)\n",
    "        self.conv4 = nn.Conv1d(n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.bn4 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.pool4 = nn.MaxPool1d(3)\n",
    "        self.conv5 = nn.Conv1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.bn5 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.pool5 = nn.MaxPool1d(3)\n",
    "        self.conv6 = nn.Conv1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.bn6 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.pool6 = nn.MaxPool1d(3)\n",
    "        self.fc1 = nn.Linear(2 * n_channel, n_channel)\n",
    "        self.fc2 = nn.Linear(n_channel, n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(f'CONV1 INPUT SHAPE: {x.shape}')\n",
    "        x = self.conv1(x)\n",
    "        # print(f'CONV1 OUTPUT SHAPE: {x.shape}')\n",
    "        x = F.relu(self.bn1(x))\n",
    "        # print(f'POOL1 INPUT SHAPE: {x.shape}')\n",
    "        x = self.pool1(x)\n",
    "        # print(f'POOL1 OUTPUT SHAPE: {x.shape}')\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.bn2(x))\n",
    "        # print(f'POOL2 INPUT SHAPE: {x.shape}')\n",
    "        x = self.pool2(x)\n",
    "        # print(f'POOL2 OUTPUT SHAPE: {x.shape}')\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(self.bn3(x))\n",
    "        # print(f'POOL3 INPUT SHAPE: {x.shape}')\n",
    "        x = self.pool3(x)\n",
    "        # print(f'POOL3 OUTPUT SHAPE: {x.shape}')\n",
    "        x = self.conv4(x)\n",
    "        # print(f'BATCHNORM4 INPUT SHAPE: {x.shape}')\n",
    "        x = F.relu(self.bn4(x))\n",
    "        # print(f'POOL4 INPUT SHAPE: {x.shape}')\n",
    "        x = self.pool4(x)\n",
    "        # print(f'POOL4 OUTPUT SHAPE: {x.shape}')\n",
    "        x = self.conv5(x)\n",
    "        # print(f'BATCHNORM5 INPUT SHAPE: {x.shape}')\n",
    "        x = F.relu(self.bn5(x))\n",
    "        # print(f'POOL5 INPUT SHAPE: {x.shape}')\n",
    "        x = self.pool5(x)\n",
    "        # print(f'POOL5 OUTPUT SHAPE: {x.shape}')\n",
    "        x = self.conv6(x)\n",
    "        # print(f'BATCHNORM6 INPUT SHAPE: {x.shape}')\n",
    "        x = F.relu(self.bn6(x))\n",
    "        # print(f'POOL6 INPUT SHAPE: {x.shape}')\n",
    "        x = self.pool6(x)\n",
    "        # print(f'POOL6 OUTPUT SHAPE: {x.shape}')\n",
    "        x = F.avg_pool1d(x, x.shape[-1])\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=2)\n",
    "\n",
    "\n",
    "model = M5(n_input = 1, n_output = len(set(dataset.labels)))\n",
    "model.double().to(device)\n",
    "print(model)\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "n = count_parameters(model)\n",
    "print(\"Number of parameters: %s\" % n)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=3e-4, weight_decay=0.0001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.7)  # reduce the learning after 20 epochs by a factor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def train(model, epoch, log_interval, writer: SummaryWriter):\n",
    "    train_losses = []\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        data = data.to(device)\n",
    "        # print(f'DATA SHAPE: {data.shape}')\n",
    "        target = target.to(device)\n",
    "\n",
    "        # apply transform and model on whole batch directly on device\n",
    "        output = model(data)\n",
    "\n",
    "        # negative log-likelihood for a tensor of size (batch x 1 x n_output)\n",
    "        squeezed_output = output.squeeze()\n",
    "        loss = F.nll_loss(squeezed_output, target.argmax(dim = 1))\n",
    "\n",
    "        writer.add_scalar('Train loss', loss.item(), epoch * len(train_loader.dataset) + batch_idx)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print training stats\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\")\n",
    "\n",
    "        # update progress bar\n",
    "        pbar.update(pbar_update)\n",
    "        # record loss\n",
    "        train_losses.append(loss.item())\n",
    "    return train_losses"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def number_of_correct(pred, target):\n",
    "    # count number of correct predictions\n",
    "    return pred.squeeze().eq(target).sum().item()\n",
    "\n",
    "\n",
    "def get_likely_index(tensor):\n",
    "    # find most likely label index for each element in the batch\n",
    "    return tensor.argmax(dim=-1)\n",
    "\n",
    "\n",
    "def test(model, epoch, writer: SummaryWriter):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for data, target in test_loader:\n",
    "\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        output = model(data)\n",
    "\n",
    "        pred = get_likely_index(output)\n",
    "        correct += number_of_correct(pred, target.argmax(dim = 1))\n",
    "\n",
    "        y_true.extend(pred.squeeze().data.cpu().numpy())\n",
    "        y_pred.extend(target.data.cpu().numpy().argmax(axis = 1))\n",
    "\n",
    "        # update progress bar\n",
    "        pbar.update(pbar_update)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    writer.add_scalar('Test accuracy', accuracy, epoch)\n",
    "\n",
    "    # Build confusion matrix\n",
    "    cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    df_cm = pd.DataFrame(cf_matrix, index = [i for i in CLASSES],\n",
    "                         columns = [i for i in CLASSES])\n",
    "    plt.figure(figsize = (12,7))\n",
    "    cf_matrix_figure = sn.heatmap(df_cm, annot=True).get_figure()\n",
    "    writer.add_figure('Test confusion matrix', cf_matrix_figure, epoch)\n",
    "\n",
    "    print(f\"\\nTest Epoch: {epoch}\\tAccuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.0f}%)\\n\")\n",
    "    return accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 0.375/15 [00:00<00:19,  1.32s/it]               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/296 (0%)]\tLoss: 2.033857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▊         | 1.2916666666666667/15 [00:01<00:08,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Epoch: 1\tAccuracy: 14/74 (19%)\n",
      "\n",
      "Train Epoch: 2 [0/296 (0%)]\tLoss: 1.473974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 2.375/15 [00:01<00:05,  2.16it/s]             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Epoch: 2\tAccuracy: 37/74 (50%)\n",
      "\n",
      "Train Epoch: 3 [0/296 (0%)]\tLoss: 1.261408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 3.2916666666666634/15 [00:01<00:05,  2.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Epoch: 3\tAccuracy: 58/74 (78%)\n",
      "\n",
      "Train Epoch: 4 [0/296 (0%)]\tLoss: 1.064102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 4.416666666666663/15 [00:02<00:04,  2.26it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Epoch: 4\tAccuracy: 73/74 (99%)\n",
      "\n",
      "Train Epoch: 5 [0/296 (0%)]\tLoss: 0.780762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 5.3750000000000036/15 [00:02<00:04,  2.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Epoch: 5\tAccuracy: 74/74 (100%)\n",
      "\n",
      "Train Epoch: 6 [0/296 (0%)]\tLoss: 0.642251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 6.291666666666677/15 [00:03<00:03,  2.28it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Epoch: 6\tAccuracy: 74/74 (100%)\n",
      "\n",
      "Train Epoch: 7 [0/296 (0%)]\tLoss: 0.699743\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 7.333333333333351/15 [00:03<00:03,  1.96it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Epoch: 7\tAccuracy: 74/74 (100%)\n",
      "\n",
      "Train Epoch: 8 [0/296 (0%)]\tLoss: 0.479421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 8.375000000000016/15 [00:04<00:02,  2.28it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Epoch: 8\tAccuracy: 74/74 (100%)\n",
      "\n",
      "Train Epoch: 9 [0/296 (0%)]\tLoss: 0.391883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 9.333333333333336/15 [00:04<00:02,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Epoch: 9\tAccuracy: 74/74 (100%)\n",
      "\n",
      "Train Epoch: 10 [0/296 (0%)]\tLoss: 0.256844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▉   | 10.333333333333321/15 [00:05<00:02,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Epoch: 10\tAccuracy: 74/74 (100%)\n",
      "\n",
      "Train Epoch: 11 [0/296 (0%)]\tLoss: 0.195701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 11.333333333333307/15 [00:05<00:01,  2.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Epoch: 11\tAccuracy: 74/74 (100%)\n",
      "\n",
      "Train Epoch: 12 [0/296 (0%)]\tLoss: 0.156645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 12.37499999999996/15 [00:05<00:01,  2.48it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Epoch: 12\tAccuracy: 74/74 (100%)\n",
      "\n",
      "Train Epoch: 13 [0/296 (0%)]\tLoss: 0.109588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 13.374999999999945/15 [00:06<00:00,  2.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Epoch: 13\tAccuracy: 74/74 (100%)\n",
      "\n",
      "Train Epoch: 14 [0/296 (0%)]\tLoss: 0.162979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 14.37499999999993/15 [00:06<00:00,  2.52it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Epoch: 14\tAccuracy: 74/74 (100%)\n",
      "\n",
      "Train Epoch: 15 [0/296 (0%)]\tLoss: 0.090300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 14.999999999999922/15 [00:06<00:00,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Epoch: 15\tAccuracy: 74/74 (100%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter()\n",
    "\n",
    "log_interval = 20\n",
    "\n",
    "writer.add_hparams({f'data_shape_{i}': shape for i, shape in enumerate(dataset.data.shape)} | {'data_moving_average_range': MOVING_AVERAGE_RANGE, 'data_window_size': WINDOW_SIZE, 'batch_size': batch_size, 'n_epoch': n_epoch}, {'hparam/fake_accuracy_just_to_have_any_metric': 10}, run_name = RUN_NAME)\n",
    "\n",
    "pbar_update = 1 / (len(train_loader) + len(test_loader))\n",
    "losses = []\n",
    "accuracies = []\n",
    "\n",
    "with tqdm(total=n_epoch) as pbar:\n",
    "    for epoch in range(1, n_epoch + 1):\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict()\n",
    "        }, CHECKPOINT_PATH)\n",
    "\n",
    "        train_losses = train(model, epoch, log_interval, writer)\n",
    "        losses.extend(train_losses)\n",
    "\n",
    "        accuracy = test(model, epoch, writer)\n",
    "        accuracies.append(accuracy)\n",
    "        scheduler.step()\n",
    "\n",
    "        # Early stopping\n",
    "        if len(accuracies) >= ACCURACY_MOVING_AVERAGE_SIZE + 1:\n",
    "            is_performance_degraded = np.mean(accuracies[-ACCURACY_MOVING_AVERAGE_SIZE - 1:-1]) > np.mean(accuracies[-ACCURACY_MOVING_AVERAGE_SIZE:])\n",
    "            if is_performance_degraded:\n",
    "                # Reload the last non-degraded checkpoint\n",
    "                checkpoint = torch.load(CHECKPOINT_PATH)\n",
    "                model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "                break\n",
    "\n",
    "\n",
    "# Let's plot the training loss versus the number of iteration.\n",
    "# plt.plot(losses);\n",
    "# plt.title(\"training loss\");"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}