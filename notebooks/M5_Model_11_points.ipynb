{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from common import *\n",
    "from dataset import ArrhythmiaDataset\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "RECORD_DIR_PATH = '../data/mit-bih-arrhythmia-database-1.0.0'\n",
    "WINDOW_SIZE = 540\n",
    "INCLUDE_MANUAL_LABELS = True\n",
    "SUBSET_FROM_MANUAL_LABELS = True\n",
    "INCLUDE_RAW_SIGNAL = False\n",
    "CLASSES = ['N', 'L', 'R', 'a', 'V', 'J', 'F'] if SUBSET_FROM_MANUAL_LABELS else ['N', 'L', 'R', 'A', 'a', 'V', 'j', 'J', 'E', 'f', 'F', '[', '!', ']', '/', 'x', '|', 'Q']\n",
    "\n",
    "RUN_NAME = '11_points'\n",
    "CHECKPOINT_PATH = f'../models/{RUN_NAME} - checkpoint.pt'\n",
    "ACCURACY_MOVING_AVERAGE_SIZE = 30  # moving average for accuracy to check if performance degraded\n",
    "\n",
    "n_epoch = 15\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# Classes: 'N', 'L', 'R', 'A', 'a', 'V', 'j', 'J', 'E', 'f', 'F', '[', '!', ']', '/', 'x', '|', 'Q', 'S', 'e'\n",
    "# TODO: S, e - need some preprocessing, dimensions seem to be wrong in one of these\n",
    "# TODO: Q - of course, quite confusing, this is the most confused beat in confusion matrices\n",
    "# TODO: Manual labeled samples seem to have R-peak dislocated with respect to original dataset; this MUST not happen, as original dataset has them marked and corrected by cardiologists over 30 years"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Randomness seed\n",
    "random_seed = 1 # or any of your favorite number\n",
    "torch.manual_seed(random_seed)\n",
    "torch.cuda.manual_seed(random_seed)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(random_seed)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filename='V.csv' Unique_keys=50\n",
      "filename='L.csv' Unique_keys=100\n",
      "filename='R.csv' Unique_keys=150\n",
      "filename='J.csv' Unique_keys=218\n",
      "filename='a.csv' Unique_keys=271\n",
      "filename='F.csv' Unique_keys=321\n",
      "filename='N.csv' Unique_keys=371\n",
      "filename='100.atr' patient_record_number=100\n",
      "beat_slice_array.shape=(38, 5, 1080) beat_slices.shape=torch.Size([38, 5, 1080])\n",
      "filename='124.atr' patient_record_number=124\n",
      "beat_slice_array.shape=(20, 5, 1080) beat_slices.shape=torch.Size([20, 5, 1080])\n",
      "self.data.shape=torch.Size([38, 5, 1080]) beat_slices.shape=torch.Size([20, 5, 1080])\n",
      "filename='111.atr' patient_record_number=111\n",
      "beat_slice_array.shape=(50, 5, 1080) beat_slices.shape=torch.Size([50, 5, 1080])\n",
      "self.data.shape=torch.Size([58, 5, 1080]) beat_slices.shape=torch.Size([50, 5, 1080])\n",
      "filename='208.atr' patient_record_number=208\n",
      "beat_slice_array.shape=(50, 5, 1080) beat_slices.shape=torch.Size([50, 5, 1080])\n",
      "self.data.shape=torch.Size([108, 5, 1080]) beat_slices.shape=torch.Size([50, 5, 1080])\n",
      "filename='101.atr' patient_record_number=101\n",
      "beat_slice_array.shape=(11, 5, 1080) beat_slices.shape=torch.Size([11, 5, 1080])\n",
      "self.data.shape=torch.Size([158, 5, 1080]) beat_slices.shape=torch.Size([11, 5, 1080])\n",
      "filename='201.atr' patient_record_number=201\n",
      "beat_slice_array.shape=(53, 5, 1080) beat_slices.shape=torch.Size([53, 5, 1080])\n",
      "self.data.shape=torch.Size([169, 5, 1080]) beat_slices.shape=torch.Size([53, 5, 1080])\n",
      "filename='118.atr' patient_record_number=118\n",
      "beat_slice_array.shape=(50, 5, 1080) beat_slices.shape=torch.Size([50, 5, 1080])\n",
      "self.data.shape=torch.Size([222, 5, 1080]) beat_slices.shape=torch.Size([50, 5, 1080])\n",
      "filename='200.atr' patient_record_number=200\n",
      "beat_slice_array.shape=(50, 5, 1080) beat_slices.shape=torch.Size([50, 5, 1080])\n",
      "self.data.shape=torch.Size([272, 5, 1080]) beat_slices.shape=torch.Size([50, 5, 1080])\n",
      "filename='234.atr' patient_record_number=234\n",
      "beat_slice_array.shape=(48, 5, 1080) beat_slices.shape=torch.Size([48, 5, 1080])\n",
      "self.data.shape=torch.Size([322, 5, 1080]) beat_slices.shape=torch.Size([48, 5, 1080])\n",
      "torch.Size([370, 5, 1080])\n",
      "370\n"
     ]
    }
   ],
   "source": [
    "dataset = ArrhythmiaDataset(RECORD_DIR_PATH, WINDOW_SIZE, only_include_labels = CLASSES, include_manual_labels = INCLUDE_MANUAL_LABELS, subset_from_manual_labels = SUBSET_FROM_MANUAL_LABELS, include_raw_signal = INCLUDE_RAW_SIGNAL)\n",
    "\n",
    "print(dataset.data.shape)\n",
    "print(len(dataset.labels))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R: 50\n",
      "L: 50\n",
      "a: 53\n",
      "J: 68\n",
      "V: 50\n",
      "F: 50\n",
      "N: 49\n"
     ]
    }
   ],
   "source": [
    "labels, counts = torch.unique(dataset.labels_encoded, dim = 0, return_counts = True)\n",
    "\n",
    "for label, count in zip(labels, counts):\n",
    "    print(f'{dataset.get_label_from_tensor(label)}: {count}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([370, 5, 1080])\n",
      "370\n",
      "R: 50\n",
      "L: 50\n",
      "a: 53\n",
      "J: 68\n",
      "V: 50\n",
      "F: 50\n",
      "N: 49\n"
     ]
    }
   ],
   "source": [
    "# Drop some Normal beats to balance classes\n",
    "normal_beat_mask = np.array(dataset.labels) == 'N'\n",
    "\n",
    "FRAC_OF_NORMAL_TO_KEEP = 1.0\n",
    "\n",
    "new_labels = []\n",
    "for idx, l in enumerate(normal_beat_mask):\n",
    "    # TODO: Change this when there's more samples\n",
    "    if l and random.uniform(0, 1) < FRAC_OF_NORMAL_TO_KEEP:\n",
    "        normal_beat_mask[idx] = False\n",
    "    if not normal_beat_mask[idx]:\n",
    "        new_labels.append(dataset.labels[idx])\n",
    "\n",
    "new_data = dataset.data[normal_beat_mask == False]\n",
    "dataset.data = new_data\n",
    "dataset.labels = new_labels\n",
    "dataset.encode_labels()\n",
    "\n",
    "def show_class_count(dataset: ArrhythmiaDataset):\n",
    "    print(dataset.data.shape)\n",
    "    print(len(dataset.labels))\n",
    "    labels, counts = torch.unique(dataset.labels_encoded, dim = 0, return_counts = True)\n",
    "\n",
    "    for label, count in zip(labels, counts):\n",
    "        print(f'{dataset.get_label_from_tensor(label)}: {count}')\n",
    "\n",
    "show_class_count(dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN DATASET:\n",
      "torch.Size([296, 5, 1080])\n",
      "0\n",
      "R: 40\n",
      "L: 40\n",
      "a: 42\n",
      "J: 55\n",
      "V: 40\n",
      "F: 40\n",
      "N: 39\n",
      "TEST DATASET:\n",
      "torch.Size([74, 5, 1080])\n",
      "0\n",
      "R: 10\n",
      "L: 10\n",
      "a: 11\n",
      "J: 13\n",
      "V: 10\n",
      "F: 10\n",
      "N: 10\n"
     ]
    }
   ],
   "source": [
    "def collate_fn(batch):\n",
    "\n",
    "    # A data tuple has the form:\n",
    "    # waveform, one-hot-encoded_label\n",
    "    tensors, targets = [], []\n",
    "\n",
    "    # Gather in lists, and encode labels as indices\n",
    "    for waveform, label in batch:\n",
    "        tensors += [waveform]\n",
    "        targets += [label]\n",
    "\n",
    "    # Group the list of tensors into a batched tensor\n",
    "    tensors = torch.stack(tensors)\n",
    "    tensors = tensors[:, :]\n",
    "    targets = torch.stack(targets)\n",
    "\n",
    "    return tensors, targets\n",
    "\n",
    "\n",
    "if device == \"cuda\":\n",
    "    num_workers = 1\n",
    "    pin_memory = True\n",
    "else:\n",
    "    num_workers = 0\n",
    "    pin_memory = False\n",
    "\n",
    "train_dataset, test_dataset = dataset.train_test_split(0.2)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=pin_memory,\n",
    ")\n",
    "\n",
    "print('TRAIN DATASET:')\n",
    "show_class_count(train_dataset)\n",
    "\n",
    "print('TEST DATASET:')\n",
    "show_class_count(test_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M5(\n",
      "  (conv1): Conv1d(5, 32, kernel_size=(3,), stride=(1,))\n",
      "  (bn1): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv1d(32, 32, kernel_size=(3,), stride=(1,))\n",
      "  (bn2): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv1d(32, 32, kernel_size=(3,), stride=(1,))\n",
      "  (bn3): BatchNorm1d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool3): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv4): Conv1d(32, 64, kernel_size=(3,), stride=(1,))\n",
      "  (bn4): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool4): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv5): Conv1d(64, 64, kernel_size=(3,), stride=(1,))\n",
      "  (bn5): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool5): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv6): Conv1d(64, 64, kernel_size=(3,), stride=(1,))\n",
      "  (bn6): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool6): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=64, out_features=32, bias=True)\n",
      "  (fc2): Linear(in_features=32, out_features=7, bias=True)\n",
      ")\n",
      "Number of parameters: 40519\n"
     ]
    }
   ],
   "source": [
    "class M5(nn.Module):\n",
    "    def __init__(self, n_input=1, n_output=35, stride=1, n_channel=32):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv1d(n_input, n_channel, kernel_size=3, stride=stride)\n",
    "        self.bn1 = nn.BatchNorm1d(n_channel)\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "        self.conv2 = nn.Conv1d(n_channel, n_channel, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm1d(n_channel)\n",
    "        self.pool2 = nn.MaxPool1d(2)\n",
    "        self.conv3 = nn.Conv1d(n_channel, n_channel, kernel_size=3)\n",
    "        self.bn3 = nn.BatchNorm1d(n_channel)\n",
    "        self.pool3 = nn.MaxPool1d(3)\n",
    "        self.conv4 = nn.Conv1d(n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.bn4 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.pool4 = nn.MaxPool1d(3)\n",
    "        self.conv5 = nn.Conv1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.bn5 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.pool5 = nn.MaxPool1d(3)\n",
    "        self.conv6 = nn.Conv1d(2 * n_channel, 2 * n_channel, kernel_size=3)\n",
    "        self.bn6 = nn.BatchNorm1d(2 * n_channel)\n",
    "        self.pool6 = nn.MaxPool1d(3)\n",
    "        self.fc1 = nn.Linear(2 * n_channel, n_channel)\n",
    "        self.fc2 = nn.Linear(n_channel, n_output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # print(f'CONV1 INPUT SHAPE: {x.shape}')\n",
    "        x = self.conv1(x)\n",
    "        # print(f'CONV1 OUTPUT SHAPE: {x.shape}')\n",
    "        x = F.relu(self.bn1(x))\n",
    "        # print(f'POOL1 INPUT SHAPE: {x.shape}')\n",
    "        x = self.pool1(x)\n",
    "        # print(f'POOL1 OUTPUT SHAPE: {x.shape}')\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(self.bn2(x))\n",
    "        # print(f'POOL2 INPUT SHAPE: {x.shape}')\n",
    "        x = self.pool2(x)\n",
    "        # print(f'POOL2 OUTPUT SHAPE: {x.shape}')\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(self.bn3(x))\n",
    "        # print(f'POOL3 INPUT SHAPE: {x.shape}')\n",
    "        x = self.pool3(x)\n",
    "        # print(f'POOL3 OUTPUT SHAPE: {x.shape}')\n",
    "        x = self.conv4(x)\n",
    "        # print(f'BATCHNORM4 INPUT SHAPE: {x.shape}')\n",
    "        x = F.relu(self.bn4(x))\n",
    "        # print(f'POOL4 INPUT SHAPE: {x.shape}')\n",
    "        x = self.pool4(x)\n",
    "        # print(f'POOL4 OUTPUT SHAPE: {x.shape}')\n",
    "        x = self.conv5(x)\n",
    "        # print(f'BATCHNORM5 INPUT SHAPE: {x.shape}')\n",
    "        x = F.relu(self.bn5(x))\n",
    "        # print(f'POOL5 INPUT SHAPE: {x.shape}')\n",
    "        x = self.pool5(x)\n",
    "        # print(f'POOL5 OUTPUT SHAPE: {x.shape}')\n",
    "        x = self.conv6(x)\n",
    "        # print(f'BATCHNORM6 INPUT SHAPE: {x.shape}')\n",
    "        x = F.relu(self.bn6(x))\n",
    "        # print(f'POOL6 INPUT SHAPE: {x.shape}')\n",
    "        x = self.pool6(x)\n",
    "        # print(f'POOL6 OUTPUT SHAPE: {x.shape}')\n",
    "        x = F.avg_pool1d(x, x.shape[-1])\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=2)\n",
    "\n",
    "\n",
    "model = M5(n_input=5, n_output=len(set(dataset.labels)))\n",
    "model.double().to(device)\n",
    "print(model)\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "\n",
    "n = count_parameters(model)\n",
    "print(\"Number of parameters: %s\" % n)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=3e-4, weight_decay=0.0001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.7)  # reduce the learning after 20 epochs by a factor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def number_of_correct(pred, target):\n",
    "    # count number of correct predictions\n",
    "    return pred.squeeze().eq(target).sum().item()\n",
    "\n",
    "\n",
    "def get_likely_index(tensor):\n",
    "    # find most likely label index for each element in the batch\n",
    "    return tensor.argmax(dim=-1)\n",
    "\n",
    "def train(model, epoch, log_interval, writer: SummaryWriter):\n",
    "    train_losses = []\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        writer.add_hparams({'lr': scheduler.get_last_lr()[-1]}, {'hparam/fake_accuracy_just_to_have_any_metric': 10}, run_name = RUN_NAME)\n",
    "\n",
    "        data = data.to(device)\n",
    "        # print(f'DATA SHAPE: {data.shape}')\n",
    "        target = target.to(device)\n",
    "\n",
    "        # apply transform and model on whole batch directly on device\n",
    "        output = model(data)\n",
    "\n",
    "        # negative log-likelihood for a tensor of size (batch x 1 x n_output)\n",
    "        squeezed_output = output.squeeze()\n",
    "        loss = F.nll_loss(squeezed_output, target.argmax(dim = 1))\n",
    "\n",
    "        writer.add_scalar('Train loss', loss.item(), epoch * len(train_loader.dataset) + batch_idx)\n",
    "\n",
    "        # Train accuracy\n",
    "        pred = get_likely_index(output)\n",
    "        correct += number_of_correct(pred, target.argmax(dim = 1))\n",
    "\n",
    "        y_true.extend(pred.squeeze().data.cpu().numpy())\n",
    "        y_pred.extend(target.data.cpu().numpy().argmax(axis = 1))\n",
    "\n",
    "        # Backprop\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print training stats\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(f\"Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}\")\n",
    "\n",
    "        # update progress bar\n",
    "        pbar.update(pbar_update)\n",
    "        # record loss\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    accuracy = 100. * correct / len(train_loader.dataset)\n",
    "    writer.add_scalar('Train accuracy', accuracy, epoch)\n",
    "\n",
    "    # Build confusion matrix\n",
    "    cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    df_cm = pd.DataFrame(cf_matrix, index = [i for i in CLASSES],\n",
    "                         columns = [i for i in CLASSES])\n",
    "    plt.figure(figsize = (12,7))\n",
    "    cf_matrix_figure = sn.heatmap(df_cm, annot=True).get_figure()\n",
    "    writer.add_figure('Train confusion matrix', cf_matrix_figure, epoch)\n",
    "\n",
    "    print(f'\\nTrain Epoch: {epoch}\\tAccuracy: {correct}/{len(train_loader.dataset)} ({accuracy:.0f}%)\\n')\n",
    "    return train_losses"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def test(model, epoch, writer: SummaryWriter):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    for data, target in test_loader:\n",
    "\n",
    "        data = data.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        output = model(data)\n",
    "\n",
    "        pred = get_likely_index(output)\n",
    "        correct += number_of_correct(pred, target.argmax(dim = 1))\n",
    "\n",
    "        y_true.extend(pred.squeeze().data.cpu().numpy())\n",
    "        y_pred.extend(target.data.cpu().numpy().argmax(axis = 1))\n",
    "\n",
    "        # update progress bar\n",
    "        pbar.update(pbar_update)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    writer.add_scalar('Test accuracy', accuracy, epoch)\n",
    "\n",
    "    # Build confusion matrix\n",
    "    cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    df_cm = pd.DataFrame(cf_matrix, index = [i for i in CLASSES],\n",
    "                         columns = [i for i in CLASSES])\n",
    "    plt.figure(figsize = (12,7))\n",
    "    cf_matrix_figure = sn.heatmap(df_cm, annot=True).get_figure()\n",
    "    writer.add_figure('Test confusion matrix', cf_matrix_figure, epoch)\n",
    "\n",
    "    print(f\"\\nTest Epoch: {epoch}\\tAccuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.0f}%)\\n\")\n",
    "    return accuracy"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 0.3333333333333333/15 [00:00<00:19,  1.35s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/296 (0%)]\tLoss: 2.011876\n",
      "Train Epoch: 1 [80/296 (26%)]\tLoss: 1.787183\n",
      "Train Epoch: 1 [160/296 (53%)]\tLoss: 1.664308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 0.8749999999999997/15 [00:00<00:11,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [240/296 (79%)]\tLoss: 1.665658\n",
      "\n",
      "Train Epoch: 1\tAccuracy: 107/296 (36%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 1.3333333333333335/15 [00:01<00:09,  1.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Epoch: 1\tAccuracy: 11/74 (15%)\n",
      "\n",
      "Train Epoch: 2 [0/296 (0%)]\tLoss: 1.486929\n",
      "Train Epoch: 2 [80/296 (26%)]\tLoss: 1.444639\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1.583333333333334/15 [00:01<00:07,  1.72it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 2 [160/296 (53%)]\tLoss: 1.253546\n",
      "Train Epoch: 2 [240/296 (79%)]\tLoss: 1.260434\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 2.041666666666668/15 [00:01<00:08,  1.51it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Epoch: 2\tAccuracy: 245/296 (83%)\n",
      "\n",
      "\n",
      "Test Epoch: 2\tAccuracy: 20/74 (27%)\n",
      "\n",
      "Train Epoch: 3 [0/296 (0%)]\tLoss: 1.040862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 2.541666666666666/15 [00:01<00:06,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 3 [80/296 (26%)]\tLoss: 1.187756\n",
      "Train Epoch: 3 [160/296 (53%)]\tLoss: 1.057813\n",
      "Train Epoch: 3 [240/296 (79%)]\tLoss: 0.939736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 3.0416666666666643/15 [00:02<00:07,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Epoch: 3\tAccuracy: 272/296 (92%)\n",
      "\n",
      "\n",
      "Test Epoch: 3\tAccuracy: 74/74 (100%)\n",
      "\n",
      "Train Epoch: 4 [0/296 (0%)]\tLoss: 0.950098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▎       | 3.5416666666666625/15 [00:02<00:05,  1.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 4 [80/296 (26%)]\tLoss: 0.891559\n",
      "Train Epoch: 4 [160/296 (53%)]\tLoss: 0.801685\n",
      "Train Epoch: 4 [240/296 (79%)]\tLoss: 0.636094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 4.041666666666661/15 [00:02<00:07,  1.49it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Epoch: 4\tAccuracy: 289/296 (98%)\n",
      "\n",
      "\n",
      "Test Epoch: 4\tAccuracy: 74/74 (100%)\n",
      "\n",
      "Train Epoch: 5 [0/296 (0%)]\tLoss: 0.547905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 4.583333333333331/15 [00:03<00:05,  1.88it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 5 [80/296 (26%)]\tLoss: 0.667064\n",
      "Train Epoch: 5 [160/296 (53%)]\tLoss: 0.655821\n",
      "Train Epoch: 5 [240/296 (79%)]\tLoss: 0.493127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▎      | 5.041666666666668/15 [00:03<00:06,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Epoch: 5\tAccuracy: 296/296 (100%)\n",
      "\n",
      "\n",
      "Test Epoch: 5\tAccuracy: 74/74 (100%)\n",
      "\n",
      "Train Epoch: 6 [0/296 (0%)]\tLoss: 0.434236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 5.541666666666671/15 [00:03<00:05,  1.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 6 [80/296 (26%)]\tLoss: 0.411423\n",
      "Train Epoch: 6 [160/296 (53%)]\tLoss: 0.460314\n",
      "Train Epoch: 6 [240/296 (79%)]\tLoss: 0.350878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 6.041666666666675/15 [00:03<00:06,  1.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Epoch: 6\tAccuracy: 295/296 (100%)\n",
      "\n",
      "\n",
      "Test Epoch: 6\tAccuracy: 74/74 (100%)\n",
      "\n",
      "Train Epoch: 7 [0/296 (0%)]\tLoss: 0.290584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 6.5833333333333455/15 [00:04<00:04,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 7 [80/296 (26%)]\tLoss: 0.384545\n",
      "Train Epoch: 7 [160/296 (53%)]\tLoss: 0.239410\n",
      "Train Epoch: 7 [240/296 (79%)]\tLoss: 0.255340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 7.041666666666682/15 [00:04<00:05,  1.52it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Epoch: 7\tAccuracy: 296/296 (100%)\n",
      "\n",
      "\n",
      "Test Epoch: 7\tAccuracy: 74/74 (100%)\n",
      "\n",
      "Train Epoch: 8 [0/296 (0%)]\tLoss: 0.259827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 7.541666666666686/15 [00:04<00:04,  1.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 8 [80/296 (26%)]\tLoss: 0.222658\n",
      "Train Epoch: 8 [160/296 (53%)]\tLoss: 0.265092\n",
      "Train Epoch: 8 [240/296 (79%)]\tLoss: 0.177679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 7.791666666666687/15 [00:04<00:03,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Epoch: 8\tAccuracy: 296/296 (100%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 8.291666666666684/15 [00:05<00:04,  1.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test Epoch: 8\tAccuracy: 74/74 (100%)\n",
      "\n",
      "Train Epoch: 9 [0/296 (0%)]\tLoss: 0.179516\n",
      "Train Epoch: 9 [80/296 (26%)]\tLoss: 0.160119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 8.791666666666677/15 [00:05<00:03,  1.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 9 [160/296 (53%)]\tLoss: 0.149986\n",
      "Train Epoch: 9 [240/296 (79%)]\tLoss: 0.156860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 9.000000000000007/15 [00:05<00:03,  1.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Epoch: 9\tAccuracy: 296/296 (100%)\n",
      "\n",
      "\n",
      "Test Epoch: 9\tAccuracy: 74/74 (100%)\n",
      "\n",
      "Train Epoch: 10 [0/296 (0%)]\tLoss: 0.123088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 9.458333333333334/15 [00:06<00:03,  1.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 10 [80/296 (26%)]\tLoss: 0.119410\n",
      "Train Epoch: 10 [160/296 (53%)]\tLoss: 0.120016\n",
      "Train Epoch: 10 [240/296 (79%)]\tLoss: 0.146230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 10.166666666666657/15 [00:06<00:03,  1.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Epoch: 10\tAccuracy: 296/296 (100%)\n",
      "\n",
      "\n",
      "Test Epoch: 10\tAccuracy: 74/74 (100%)\n",
      "\n",
      "Train Epoch: 11 [0/296 (0%)]\tLoss: 0.117060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 10.749999999999982/15 [00:06<00:02,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 11 [80/296 (26%)]\tLoss: 0.105803\n",
      "Train Epoch: 11 [160/296 (53%)]\tLoss: 0.085718\n",
      "Train Epoch: 11 [240/296 (79%)]\tLoss: 0.079876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 10.999999999999979/15 [00:06<00:02,  1.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Epoch: 11\tAccuracy: 296/296 (100%)\n",
      "\n",
      "\n",
      "Test Epoch: 11\tAccuracy: 74/74 (100%)\n",
      "\n",
      "Train Epoch: 12 [0/296 (0%)]\tLoss: 0.069785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 11.749999999999968/15 [00:07<00:01,  1.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 12 [80/296 (26%)]\tLoss: 0.077433\n",
      "Train Epoch: 12 [160/296 (53%)]\tLoss: 0.067893\n",
      "Train Epoch: 12 [240/296 (79%)]\tLoss: 0.080438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 11.999999999999964/15 [00:07<00:01,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Epoch: 12\tAccuracy: 296/296 (100%)\n",
      "\n",
      "\n",
      "Test Epoch: 12\tAccuracy: 74/74 (100%)\n",
      "\n",
      "Train Epoch: 13 [0/296 (0%)]\tLoss: 0.066946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▍ | 12.749999999999954/15 [00:07<00:01,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 13 [80/296 (26%)]\tLoss: 0.079404\n",
      "Train Epoch: 13 [160/296 (53%)]\tLoss: 0.055831\n",
      "Train Epoch: 13 [240/296 (79%)]\tLoss: 0.068734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 12.99999999999995/15 [00:07<00:01,  1.87it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Epoch: 13\tAccuracy: 296/296 (100%)\n",
      "\n",
      "\n",
      "Test Epoch: 13\tAccuracy: 74/74 (100%)\n",
      "\n",
      "Train Epoch: 14 [0/296 (0%)]\tLoss: 0.072756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████▏| 13.708333333333274/15 [00:08<00:00,  1.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 14 [80/296 (26%)]\tLoss: 0.052679\n",
      "Train Epoch: 14 [160/296 (53%)]\tLoss: 0.046864\n",
      "Train Epoch: 14 [240/296 (79%)]\tLoss: 0.055105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 14.083333333333268/15 [00:08<00:00,  1.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Epoch: 14\tAccuracy: 296/296 (100%)\n",
      "\n",
      "\n",
      "Test Epoch: 14\tAccuracy: 74/74 (100%)\n",
      "\n",
      "Train Epoch: 15 [0/296 (0%)]\tLoss: 0.084303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 14.583333333333261/15 [00:09<00:00,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 15 [80/296 (26%)]\tLoss: 0.046856\n",
      "Train Epoch: 15 [160/296 (53%)]\tLoss: 0.049173\n",
      "Train Epoch: 15 [240/296 (79%)]\tLoss: 0.048199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 14.999999999999922/15 [00:09<00:00,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train Epoch: 15\tAccuracy: 296/296 (100%)\n",
      "\n",
      "\n",
      "Test Epoch: 15\tAccuracy: 74/74 (100%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "writer = SummaryWriter()\n",
    "\n",
    "log_interval = 5\n",
    "\n",
    "writer.add_hparams({f'data_shape_{i}': shape for i, shape in enumerate(dataset.data.shape)} | {'data_moving_average_range': -1, 'data_window_size': WINDOW_SIZE, 'batch_size': batch_size, 'n_epoch': n_epoch}, {'hparam/fake_accuracy_just_to_have_any_metric': 10}, run_name = RUN_NAME)\n",
    "\n",
    "pbar_update = 1 / (len(train_loader) + len(test_loader))\n",
    "losses = []\n",
    "accuracies = []\n",
    "\n",
    "with tqdm(total=n_epoch) as pbar:\n",
    "    for epoch in range(1, n_epoch + 1):\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict()\n",
    "        }, CHECKPOINT_PATH)\n",
    "\n",
    "        train_losses = train(model, epoch, log_interval, writer)\n",
    "        losses.extend(train_losses)\n",
    "\n",
    "        accuracy = test(model, epoch, writer)\n",
    "        accuracies.append(accuracy)\n",
    "        scheduler.step()\n",
    "\n",
    "        # Early stopping\n",
    "        if len(accuracies) >= ACCURACY_MOVING_AVERAGE_SIZE + 1:\n",
    "            is_performance_degraded = np.mean(accuracies[-ACCURACY_MOVING_AVERAGE_SIZE - 1:-1]) > np.mean(accuracies[-ACCURACY_MOVING_AVERAGE_SIZE:])\n",
    "            if is_performance_degraded:\n",
    "                # Reload the last non-degraded checkpoint\n",
    "                checkpoint = torch.load(CHECKPOINT_PATH)\n",
    "                model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "                break\n",
    "\n",
    "\n",
    "# Let's plot the training loss versus the number of iteration.\n",
    "# plt.plot(losses);\n",
    "# plt.title(\"training loss\");"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}